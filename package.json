{
  "name": "local-llm-chat-vscode-openmetadata",
  "displayName": "Local LLM for OpenMetadata",
  "description": "Chat with OpenMetadata using local LLMs (OpenAI, Ollama, or custom endpoints)",
  "version": "1.0.1",
  "publisher": "MarkusBegerow",
  "icon": "assets/openmetadata.png",
  "author": {
	  "name": "Markus Begerow"
  },
  "contributors": [
	  "Markus Begerow"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/markusbegerow/local-llm-chat-vscode-openmetadata.git"
  },
  "homepage": "https://github.com/markusbegerow/local-llm-chat-vscode-openmetadata#readme",
  "bugs": { "url": "https://github.com/markusbegerow/local-llm-chat-vscode-openmetadata/issues" },
  "sponsor": {
	  "url": "https://github.com/sponsors/markusbegerow"
  },
  "license": "MIT",
  "engines": {
    "vscode": "^1.74.0"
  },
  "categories": [
    "Extension Packs",
    "Other",
    "AI",
    "Chat",
    "Visualization"
  ],
  "keywords": [
    "openmetadata",
    "data-catalog",
    "lineage",
    "ai",
    "data-discovery",
    "llm",
    "openai",
    "ollama",
    "local-llm",
    "chat"
  ],
  "activationEvents": [
    "onStartupFinished"
  ],
  "main": "./dist/extension.js",
  "contributes": {
    "viewsContainers": {
      "panel": [
        {
          "id": "openmetadataPanel",
          "title": "OPEN METADATA",
          "icon": "$(database)"
        }
      ]
    },
    "views": {
      "openmetadataPanel": [
        {
          "id": "openmetadataExplorer",
          "name": "Explorer",
          "type": "webview"
        }
      ]
    },
    "configuration": {
      "title": "Local LLM Chat for OpenMetadata",
      "properties": {
        "openmetadataExplorer.openmetadataUrl": {
          "type": "string",
          "default": "http://localhost:8585",
          "description": "OpenMetadata server URL",
          "order": 1
        },
        "openmetadataExplorer.openmetadataAuthToken": {
          "type": "string",
          "description": "OpenMetadata authentication token (leave empty if auth is disabled)",
          "default": "",
          "order": 2
        },
        "openmetadataExplorer.llm.apiUrl": {
          "type": "string",
          "default": "http://localhost:11434/v1/chat/completions",
          "markdownDescription": "Full API endpoint URL.\n\n**Examples:**\n- OpenAI: `https://api.openai.com/v1/chat/completions`\n- Ollama: `http://localhost:11434/v1/chat/completions`\n- Custom: `http://localhost:1234/v1/chat/completions`",
          "order": 3
        },
        "openmetadataExplorer.llm.token": {
          "type": "string",
          "default": "ollama",
          "markdownDescription": "API authentication token.\n\n**Examples:**\n- OpenAI: `sk-your-openai-api-key-here`\n- Ollama: `ollama` (or any dummy value)\n- Custom: `your-token-or-dummy-value`",
          "order": 4
        },
        "openmetadataExplorer.llm.model": {
          "type": "string",
          "default": "llama3.2",
          "markdownDescription": "Model name to use.\n\n**Examples:**\n- OpenAI: `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`\n- Ollama: `llama3.2`, `mistral`, `codellama`\n- Custom: `your-model-name`",
          "order": 5
        },
        "openmetadataExplorer.llm.temperature": {
          "type": "number",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "markdownDescription": "Sampling temperature for model responses.\n- `0.0` = deterministic\n- `2.0` = very random",
          "order": 6
        },
        "openmetadataExplorer.llm.maxTokens": {
          "type": "number",
          "default": 2048,
          "minimum": 128,
          "maximum": 32768,
          "markdownDescription": "Maximum tokens for model responses.",
          "order": 7
        },
        "openmetadataExplorer.llm.systemPrompt": {
          "type": "string",
          "default": "You are a helpful AI assistant for analyzing data catalog metadata. Provide clear, concise insights about tables, columns, and data relationships.",
          "markdownDescription": "System prompt sent to the LLM to define its behavior.",
          "editPresentation": "multilineText",
          "order": 8
        },
        "openmetadataExplorer.llm.maxHistoryMessages": {
          "type": "number",
          "default": 50,
          "minimum": 5,
          "maximum": 200,
          "markdownDescription": "Maximum number of messages to keep in conversation history.",
          "order": 9
        },
        "openmetadataExplorer.llm.requestTimeout": {
          "type": "number",
          "default": 120000,
          "minimum": 10000,
          "maximum": 600000,
          "markdownDescription": "Request timeout in milliseconds (default: 120000 = 2 minutes).",
          "order": 10
        }
      }
    },
    "commands": [
      {
        "command": "openmetadataExplorer.refresh",
        "title": "Refresh",
        "icon": "$(refresh)"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "webpack --mode production",
    "watch": "webpack --mode development --watch",
    "package": "npm run compile && npx @vscode/vsce package",
    "pretest": "npm run compile && npm run lint",
    "lint": "eslint src --ext ts",
    "test": "node ./out/test/runTest.js"
  },
  "devDependencies": {
    "@types/node": "16.x",
    "@types/react": "^18.3.26",
    "@types/vscode": "^1.74.0",
    "@types/ws": "^8.18.1",
    "@typescript-eslint/eslint-plugin": "^5.45.0",
    "@typescript-eslint/parser": "^5.45.0",
    "css-loader": "^6.11.0",
    "eslint": "^8.28.0",
    "style-loader": "^3.3.1",
    "ts-loader": "^9.5.4",
    "typescript": "^4.9.4",
    "webpack": "^5.102.1",
    "webpack-cli": "^5.0.1"
  },
  "dependencies": {
    "@types/react-dom": "^18.0.10",
    "axios": "^1.12.2",
    "elkjs": "^0.10.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "reactflow": "^11.11.4"
  }
}
